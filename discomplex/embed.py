
import pandas as pd
from Bio import SeqIO
import biolmai # from bioml.ai for folding with ESMfold via API. Needs API key specified in file .env!
import gzip
from Bio.PDB import PDBParser, Selection, NeighborSearch
import sys
import io
import os
from dotenv import load_dotenv
from pairfolding import load_fasta_sequences
import numpy as np
import requests
import time

# Load environment variables from .env file
load_dotenv()

# Now you can use the environment variable
BIOLMAI_TOKEN = os.getenv('BIOLMAI_TOKEN')

# Define PROJECT_HOME as two levels up from the current script
PROJECT_HOME = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_DIR = f'{PROJECT_HOME}/data/raw'
INTERIM_DIR = f'{PROJECT_HOME}/data/interim'
PROCESSED_DIR = f'{PROJECT_HOME}/data/processed'


def protein_embed_esm(sequence,
    token_mode="per_token",
    model_slug = 'esm2-650m',
    base_url='https://biolm.ai/api/v2'):
    """
    Compute per-residue embeddings using a variant of the ESM-2
    model.

    Args:
        token_mode(str): either "per token" or "mean"
    """    
    url = f"{base_url}/{model_slug}/encode/"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Token {BIOLMAI_TOKEN.strip()}",
    }

    data = {
        "params": {
            "include": [
                token_mode, # "per_token" or  'mean'
                # "contacts",
                # "logits",
                # "attentions"
            ]
        },
        "items": [
            {
                "sequence": sequence, # "MSILVTRPSPAGEELVSRLRTLGQVAWHFPLIEFSPGQQLPQLADQLAALGESDLLFALSQHAVAFAQSQLHQQDRKWPRLPDYFAIGRTTALALHTVSGQKILYPQDREISEVLLQLPELQNIAGKRALILRGNGGRELIGDTLTARGAEVTFCECYQRCAIHYDGAEEAMRWQAREVTMVVVTSGEMLQQLWSLIPQWYREHWLLHCRLLVVSERLAKLARELGWQDIKVADNADNDALLRALQ",

            },
        ]
    }
    print(f"Submitting following request to {base_url}")
    print(data)
    # Make the request!
    response = requests.post(
        url=url,
        headers=headers,
        json=data,
    ).json()
    # print(response)
    print(type(response))
    if isinstance(response,dict) and 'error' in response:
        raise ValueError(f"An error raise in API call. Check format of submitted data but also status of API Token maintained at the biolm.ai user account: {response['error']}.")
    print(len(response))
    print(response.keys())
    if 'results' not in response:
        raise ValueError("Missing expected keyword 'results' in response dictionary: {")
    results = response['results']
    if not isinstance(results, list):
        raise(f"Strange, expected a 'list' datatype for response[results]: {type(results)}")
    if len(results) ==0:
        raise(f"Strange, expected at list one list item response[results]: {len(results)}")
    if len(results) > 1:
        print(f"Minor warning: only the first result will be analyzed, received {len(results)}")
    results = results[0]
    #  user can go further:   embeddings = results['representations']; logits = results['logits'] etc
    return results
    

    # Ëš

    #     cls = biolmai.ESMFoldSingleChain() if len(seqs)==1 else biolmai.ESMFoldMultiChain()
    #     if isinstance(seqs, str):
    #         seqs = [seqs]
    #     if len(seqs) > 1:
    #         seqs = [':'.join(seqs)]
    #     resp = cls.predict(seqs)[0]

    #     print("Response object after biolmai/ESMfolding API call:", type(resp))
    #     # print(resp)
    #     print(resp.keys())
    #     if 'error' in resp:
    #         print("Obtained error message:", resp['error'])
    #         return {'response':resp}
    #     results = resp['results']

    #     pdb = results[0]['pdb']
    #     pdb = pdb.replace(r'\n', '\n') # fix issue: initially newline is provided as 2 characters of \ and n, but it should be one character \n
    #     return {'pdb_text':pdb, 'response':resp}


def protein_embed(sequence,
    token_mode="per_token",
    model = {
    'name':'esm2-650m', 'family':'esm'}):
    """
    Wrapper function for potentially different methods beyond ESM
    (like AlphaFold, RosettaFold) etc.
    Currently only method 'esm' is implemented.
    """
    if 'family' not in model or model['family'] != 'esm':
        raise ValueError("Currently only method 'esm' is supported model family, but found " + model)
    return protein_embed_esm(sequence, token_mode=token_mode, model_slug=model['name'])
    


def run_embed_loop(
        accessions,
        sequence_dictionary,
        table_gt=None,
        id_col="acc", # column with ID in table table_gt
        n=1,
        seq_len_max=400,
        token_mode="per_token",
        representation_keys = { # needed for API
            'per_token':'representations',
            'mean':'mean_representations'
        },
        model = {
            'name':'esm2-650m',
            'family':'esm',
            'dim':1280

        },
        retry_max=3,
        wait_per_iteration=10.0):
    if token_mode not in representation_keys:
        raise ValueError("internal error: token_mode must be in representation_keys")
    if n <= 0:
        n = len(accessions)
    else:
        n = min(n, len(accessions))
    groundtruth_all = [] # list over all positions of all proteins visited in order
    embedding_vecs = [] # list over all embedding vectors for all positions of all visited proteins
    used_accessions = []
    used_sequences = []
    for i in range(n):
        acc = accessions[i]
        print(f"Working on accession {acc} that is row {i+1} out of {n} ({round(100*i/n,1)}% )")
        assert acc in sequence_dictionary
        seq = str(sequence_dictionary[acc].seq)
        seq_len = len(seq)
        if seq_len > seq_len_max:
            print(f"Sequence length of {acc} exceedes maximum: {seq_len} > {seq_len_max}")
            continue
        # get all disordered region for this protein chain:
        groundtruth_vec = None
        if table_gt is not None:
            ref_regions = table_gt[table_gt[id_col]== acc].reset_index(drop=True)
            groundtruth_vec = np.zeros([seq_len])
            # we are now defining an outcome vector for binary classification:
            print(ref_regions.head().to_string())
            for j in range(len(ref_regions)):
                start = ref_regions.at[j, 'start'] - 1 # back to 0-based counting for start
                assert start >= 0
                stop = ref_regions.at[j, 'end']
                assert stop <= seq_len
                groundtruth_vec[start:stop]=1.0
        success = False
        for retry in range(retry_max):
            try:
                embedding_results = protein_embed(seq, token_mode=token_mode, model=model)
                # print(embedding_results) # complete output - for debugging
                success = True
                break
            except Exception as e:
                print(f"API call resulted in exception: {e}")
                print("Retry attempt", retry + 1)
                time.sleep(wait_per_iteration)
        if not success:
            print(f"Warning: could not generate results for row {i}: {acc}")
            continue
        print(embedding_results.keys())
        representation_key = representation_keys[token_mode]
        if not isinstance(embedding_results, dict) or representation_key not in embedding_results:
            print(f"Strange, no embeddings defined for accession {acc}")
            print(embedding_results)
            print(embedding_results.keys())
            assert False
            continue
        embeddings = embedding_results[representation_key]
        if isinstance(embeddings, dict) and len(embeddings) == 1:
            _embed_keys = list(embeddings.keys()) # name of neural network layer like '33' as string
            embeddings = embeddings[_embed_keys[0]]
        # if len(embeddings) != seq_len or not isinstance(embeddings, list):
        #    raise("Inconsistent number of embeddings vectors, expected a lists os lists so that there is one embedding vector per residue")
        print("found embeddings:", type(embeddings), len(embeddings), type(embeddings[0]))
        if isinstance(embeddings[0], list):
            print("length of individual lists:", len(embeddings[0]))
        # print(embeddings)
        if isinstance(embeddings[0], list):
            for pos in range(len(embeddings)):
                embedding_vecs.append(embeddings[pos])
            if groundtruth_vec is not None:
                groundtruth_all.extend(groundtruth_vec)
        else:
            print("appending averaged embedding vector without per-position ground truth")
            embedding_vecs.append(embeddings)
        used_accessions.append({
            'accession':acc,
            'length':seq_len,
        })
        used_sequences.append(seq)
        if table_gt is not None and len(embedding_vecs) != len(groundtruth_all):
            print('Warning: The length of the ground-truth vector and the number of embedding vectors did not match for {acc}')

        time.sleep(wait_per_iteration)
    return {'accessions':used_accessions, 'sequences':used_sequences, 'embeddings':embedding_vecs, 'groundtruth':groundtruth_all}


def main(groundtruth_file = f'{RAW_DIR}/disprot/disprot_2023-12.tsv',
        sequence_file = 'data/raw/uniprot/uniprot_sprot.fasta.gz',
        data_output_file=f'{PROCESSED_DIR}/disprot/disprot_esm_embed__NUMROWS__.npz',
        n=1,
        model = {
            'name':'esm2-650m',
            'family':'esm',
            'dim':1280
        },
        token_mode="per_token",
        random_state=42 ):
    if not os.path.exists(groundtruth_file):
        raise FileNotFoundError("Could not find reference file with ground-truth from disprot: " + groundtruth_file)
    tbl_gt = pd.read_csv(groundtruth_file, sep='\t')
    if random_state is not None and random_state >= 0: # shuffle
        tbl_gt = tbl_gt.sample(frac=1.0, replace=False, random_state=random_state)
    sequence_dictionary = load_fasta_sequences(sequence_file)
    acc_uniq = list(tbl_gt['acc'].unique()) # .tolist()
    print(acc_uniq)
    print(type(acc_uniq))
    assert isinstance(acc_uniq, list)
    acc_seq_uniq= list(set(sequence_dictionary.keys()))
    acc_common = list(set(acc_uniq) & set(acc_seq_uniq))
    tbl_gt = tbl_gt[tbl_gt['acc'].isin(acc_common)].reset_index(drop=True)
    if n <= 0:
        n = len(acc_common)
    else:
        n = min(n, len(acc_common))
    results = run_embed_loop(
        accessions = acc_common,
        table_gt=tbl_gt,
        token_mode=token_mode,
        sequence_dictionary=sequence_dictionary,
        n=n,
        model=model)
    assert isinstance(results, dict)
    assert 'embeddings' in results
    assert 'groundtruth' in results
    embedding_vecs = results['embeddings']
    vector_dim = len(embedding_vecs[0]) # length of individual embedding vectors
    groundtruth_all = results['groundtruth']
    num_vectors = len(embedding_vecs)
    embedding_vecs = np.vstack(embedding_vecs)
    assert embedding_vecs.shape == (num_vectors, vector_dim)
    groundtruth_np = np.zeros( [ len(groundtruth_all), 1] )
    print("shape of groundtruth_np:", groundtruth_np.shape)
    print("'shape' of groundtruth_all list:", len(groundtruth_all), type(groundtruth_all[0]))
    assert groundtruth_np.shape == (num_vectors, 1)
    assert groundtruth_np.shape == (len(groundtruth_all), 1)
    groundtruth_np[:, 0] = groundtruth_all
    # store as npz:
    data_output_file = data_output_file.replace("_NUMROWS__", str(len(groundtruth_all)))
    outdir = os.path.dirname(data_output_file)
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    print("Storing Embedding vectors and matching ground truth in file:", data_output_file)
    print("One can load the data with:")
    print(f"data = np.load({data_output_file} )")
    print("Feature data:")
    print("X=data['X']")
    print("Ground-truth binary outcome (disprot binding site yes or now)")
    print("y=data['y']")
    np.savez(data_output_file, X=embedding_vecs, y=groundtruth_np)
    assert embedding_vecs.shape[0] == groundtruth_np.shape[0], 'Number of rows does not match between X and y!'


if __name__ == '__main__':
    main(n=100)